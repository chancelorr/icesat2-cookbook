{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61285d2-20d0-4138-aa47-6972c9232291",
   "metadata": {},
   "source": [
    "# setup \n",
    "\n",
    "Do these steps before you run this notebook!\n",
    "\n",
    "First steps: setup an environment that includes the pointCollection package\n",
    "\n",
    "> mamba env create -f wf_pointCollection.yml\n",
    "> conda activate wf_pointCollection\n",
    "> python -m ipykernel install --user --name wf_pointCollection\n",
    "\n",
    "Reload the jupyter window, and choose wf_pointCollection as your notebook environment, then go forward with this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f96907-5c43-4570-87d2-c7183d819a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sliderule import icesat2\n",
    "import sliderule\n",
    "import pointCollection as pc\n",
    "import pyproj\n",
    "from scipy import linalg\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "sliderule.set_verbose(False)\n",
    "pyproj.datadir.set_data_dir('/home/jovyan/envs/wf_pointCollection/share/proj')\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51217030-9426-4bef-9c61-765d170dab93",
   "metadata": {},
   "source": [
    "# The workflow:\n",
    "\n",
    "This is a quick-and-dirty representation of how we would map height-change rates for a small region on the ice sheet.  The workflow could be scaled up to make a map of elevation-change rate for an ice sheet with the caveats that the data volume increases dramatically toward the poles and that fitting a single elevation-change rate does not take full advantage of the data.\n",
    "\n",
    "Workflow steps:\n",
    "\n",
    "1. Read all available ATL06 data for a 40x40-km box centered on x0, y0\n",
    "2. Filter the data for atl06_quality_summary==0\n",
    "3. Select data in each 1x1 km box in the dataset, fit the selected data with a vertically moving plane\n",
    "\n",
    "The remaining cells in the notebook plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cab185-c944-47a5-945a-70b9a9bd78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xy0 = [0., -3.0e6]\n",
    "crs=3413\n",
    "W=4.e4\n",
    "pad=np.array([-W/2, W/2])\n",
    "bbox = [xy0[0] + pad[[0, 1, 1, 0, 0]], xy0[1] + pad[[0, 0, 1, 1, 0]]]\n",
    "# generate a clipping polygon in a lat/lon coordinate system\n",
    "bbox_ll = pyproj.Transformer.from_crs(pyproj.CRS(crs), \n",
    "                                      pyproj.CRS(4326))\\\n",
    "                    .transform(*bbox)\n",
    "poly = [{\"lat\":lat, \"lon\":lon} for lon, lat in zip(*bbox_ll)]\n",
    "\n",
    "save_file = f'ATL06_E{int(xy0[0]/1.e3)}_N{int(xy0[1]/1.e3)}.parquet'\n",
    "icesat2.atl06sp(\n",
    "   {\n",
    "    \"poly\": poly,\n",
    "    \"t0\":\"2018-10-13T00:00:00Z\",\n",
    "    \"t1\":\"2025-10-13T00:00:00Z\", \n",
    "   \n",
    "    \"output\": {\n",
    "      \"format\": \"parquet\",\n",
    "      \"as_geo\": True,\n",
    "      \"path\": save_file,\n",
    "      \"with_checksum\": False\n",
    "    }})\n",
    "df=gpd.read_parquet(save_file).to_crs(3413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d537498-5314-45b4-a1d6-c9e9e1d1af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pointCollection library to get access to tools to iterate over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f31f1f-9939-4884-b842-9b8e140ce6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read selected fields out of the dataframe, store them in a pointCollection.data structure\n",
    "D=pc.data().from_dict({'x':np.array(df.geometry.x),'y':np.array(df.geometry.y),'time':(np.array(df.index) - np.datetime64(datetime.datetime(2018, 1, 1))).astype(float)/1.e9/24/3600/365.25 +2018})\n",
    "D.assign({field:np.array(df[field]) for field in ['h_li','h_li_sigma', 'atl06_quality_summary']})\n",
    "# select only points for which atl06_quality_summary == 0\n",
    "D.index(D.atl06_quality_summary==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e51b9f-9208-430c-bff4-cb32c086028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make a function that fits a time-varying plane to the data for a selected set of points\n",
    "\n",
    "\n",
    "def dhdt_est(D, ind):\n",
    "    xx=D.x[ind]\n",
    "    yy=D.y[ind]\n",
    "    zz=D.h_li[ind]\n",
    "    G=np.c_[np.ones(len(ind)), D.time[ind]-2020, xx-np.mean(xx), yy-np.mean(yy)]\n",
    "    sigma = 1.e4\n",
    "    keep = np.ones(G.shape[0], dtype=bool)\n",
    "    keep_last = np.zeros_like(keep)\n",
    "    n_iterations=0\n",
    "    try:\n",
    "        # iterate fit to reduce outliers\n",
    "        while not np.all(keep_last==keep) and n_iterations  < 5:\n",
    "            n_iterations += 1\n",
    "            m=linalg.lstsq(G[keep,:], zz[keep])[0]\n",
    "            r=zz-G@m\n",
    "            sigma_r = np.nanstd(r[keep])\n",
    "            keep = np.abs(r) < 3*np.maximum(sigma, 0.1)\n",
    "        # estimate errors\n",
    "        G1 = G[keep,:]\n",
    "        Ginv = np.linalg.inv(G1.T@G1)@G1.T\n",
    "        Cm = Ginv@Ginv.T*(np.maximum(0.03**2, sigma_r**2))\n",
    "        sigma_dhdt = np.sqrt(Cm[1][1])\n",
    "        return m[1], np.sum(keep), sigma_r, sigma_dhdt\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "# apply the dh/dt function to each 2.5 km square in the data\n",
    "D_dhdt = pc.apply_bin_fn(D, fn=dhdt_est, res=1.e3, fields=['dhdt','count', 'sigma_r', 'sigma_dhdt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863525cc-02c1-440b-a5a0-c7c68c90539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results:\n",
    "\n",
    "uxy = pc.unique_by_rows(np.round(np.c_[D.x, D.y]/100)*100)\n",
    "\n",
    "\n",
    "hf, hax=plt.subplots(1, 3, figsize=[12,4], layout='constrained', sharex=True, sharey=True)\n",
    "plt.sca(hax[0])\n",
    "plt.plot(uxy[:,0], uxy[:,1],'k.', markersize=1, zorder=-3)\n",
    "plt.gca().set_title('coverage')\n",
    "\n",
    "\n",
    "plt.sca(hax[1])\n",
    "small_σ= D_dhdt.sigma_dhdt < 0.125\n",
    "plt.plot(D_dhdt.x[small_σ==0], D_dhdt.y[small_σ==0],'.', color='darkgray')\n",
    "plt.colorbar(\n",
    "    plt.scatter(D_dhdt.x[small_σ], D_dhdt.y[small_σ], 3, c=D_dhdt.dhdt[small_σ], cmap='RdBu', clim=[-1, 1]), \n",
    "    label='dh/dt, m/yr', shrink=0.5, extend='both')\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "plt.gca().set_title('dh/dt')\n",
    "\n",
    "\n",
    "plt.sca(hax[2])\n",
    "plt.colorbar(\n",
    "    plt.scatter(D_dhdt.x, D_dhdt.y, 3, c=D_dhdt.sigma_dhdt, cmap='magma', clim=[0, 1]), \n",
    "    label='dh/dt uncertainty, m/yr', shrink=0.5, extend='max')\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "plt.gca().set_title('uncertainty')\n",
    "\n",
    "for ax in hax: \n",
    "    ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659abeba-1c76-43cf-b8c2-8d8af75f5310",
   "metadata": {},
   "source": [
    "# Speed from test run:\n",
    "\n",
    "For xy0 = [0, -3.e6] in Greenland,\n",
    "* Fetching data: 28.8 s\n",
    "* Reading cache: 456 ms\n",
    "* Processing: 2.26 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc14cb-a6ea-451f-858c-65dea16c97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf_pointCollection",
   "language": "python",
   "name": "wf_pointcollection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
